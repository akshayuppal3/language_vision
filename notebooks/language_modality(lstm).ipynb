{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Average\n",
    "from keras_contrib.layers import CRF\n",
    "import keras\n",
    "# define documents\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## tokenize the text..\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "def get_tokens(sentence):\n",
    "#     tokens = nltk.word_tokenize(sentence)  # now using tweet tokenizer\n",
    "    tokens = tknzr.tokenize(sentence)\n",
    "    tokens = [token for token in tokens if (token not in stopwords and len(token) > 1)]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/samarthgoal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/samarthgoal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup env\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## load the embeddings (glove twitter and get the metrics from bi-LSTM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_path = \"/home/samarthgoal/language_vision/input/reddit/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_rage = pd.read_csv(os.path.join(dir_path,'processed_rage.csv'))\n",
    "df_happy =  pd.read_csv(os.path.join(dir_path,'processed_happy.csv'))\n",
    "df_gore =  pd.read_csv(os.path.join(dir_path,'processed_gore.csv'))\n",
    "df_creepy =  pd.read_csv(os.path.join(dir_path,'processed_creepy.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a random balances dataset of all of the categories\n",
    "length = np.min([len(df_rage),len(df_happy),len(df_creepy),len(df_gore)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_rage[:length], df_happy[:length], df_gore[:length], df_creepy[:length]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a random balances dataset of all of the categories\n",
    "Y_new = df_final['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode the output\n",
    "Y_new = le.fit_transform(Y_new) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_list = (df_final['title'].apply(get_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## join the filtered tokens back again to the keras tokenizer which would give vocalb words etc\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "sentences = [' '.join(tokens) for tokens in token_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4990"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(token_list)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2460"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for idx,tokens in enumerate(token_list):\n",
    "    if len(tokens) > max_len:\n",
    "        max_len = len(tokens)\n",
    "        index = idx\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = vocab_size\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(sentences)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = max_len\n",
    "X = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "y = Y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## now splitting into test and training data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, Y_train, Y_test =  train_test_split(X, y,test_size =0.20,random_state= 4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1968,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_path = \"/home/samarthgoal/embeddings/glove.twitter.27B.100d.txt\" ## change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the word2vec dict from the dictionary\n",
    "def get_word2vec(file_path):\n",
    "    file = open(embedding_path, \"r\")\n",
    "    if (file):\n",
    "        word2vec = dict()\n",
    "        split = file.read().splitlines()\n",
    "        for line in split:\n",
    "            key = line.split(' ',1)[0] # the first word is the key\n",
    "            value = np.array([float(val) for val in line.split(' ')[1:]])\n",
    "            word2vec[key] = value\n",
    "        return (word2vec)\n",
    "    else:\n",
    "        print(\"invalid fiel path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/samarthgoal/embeddings/glove.twitter.27B.100d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-0bd642dca0d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_word2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-47fe9e1e2e05>\u001b[0m in \u001b[0;36mget_word2vec\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# create the word2vec dict from the dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_word2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mword2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/samarthgoal/embeddings/glove.twitter.27B.100d.txt'"
     ]
    }
   ],
   "source": [
    "w2v = get_word2vec(embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-285128aa8241>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0membedding_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0membedding_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0membedding_vector\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                 \u001b[0membedding_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'w2v' is not defined"
     ]
    }
   ],
   "source": [
    "# get the embedding matrix from the embedding layer\n",
    "from numpy import zeros\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "\tembedding_vector = w2v.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1968, 34)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## main model\n",
    "input = Input(shape=(max_len,))\n",
    "model =  Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length,trainable=False)(input)\n",
    "model =  Bidirectional( LSTM(units=100,return_sequences=True,dropout=0.25),merge_mode=\"mul\")(model)\n",
    "model = TimeDistributed( Dense(100, activation=\"relu\"))(model)\n",
    "model = Flatten()(model)\n",
    "model = Dense(100, activation = 'relu')(model)\n",
    "out =  (Dense(4, activation = 'softmax'))(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## sandbox\n",
    "input = Input(shape=(max_len,))\n",
    "# model = Sequential()\n",
    "model =  Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length,trainable=False)(input)\n",
    "# model.add(e)\n",
    "model =  Bidirectional( LSTM(units=100,return_sequences=True,dropout=0.25),merge_mode=\"mul\")(model)\n",
    "# model =  Bidirectional( LSTM(units=100,return_sequences=True,dropout=0.25),merge_mode=\"concat\")(model)\n",
    "model = TimeDistributed( Dense(100, activation=\"relu\"))(model)\n",
    "# model = Lambda(lambda x: tf.reduce_mean(x, axis=1))(model)\n",
    "model = Flatten()(model)\n",
    "model = Dense(100, activation = 'relu')(model)\n",
    "out =  (Dense(4, activation = 'softmax'))(model)\n",
    "# crf = CRF(n_tags)  # CRF layer\n",
    "## @TODO add CRF and character encoding layer.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Model(input,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_112 (InputLayer)       (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "embedding_115 (Embedding)    (None, 34, 100)           499000    \n",
      "_________________________________________________________________\n",
      "bidirectional_131 (Bidirecti (None, 34, 100)           160800    \n",
      "_________________________________________________________________\n",
      "time_distributed_76 (TimeDis (None, 34, 100)           10100     \n",
      "_________________________________________________________________\n",
      "flatten_26 (Flatten)         (None, 3400)              0         \n",
      "_________________________________________________________________\n",
      "dense_180 (Dense)            (None, 100)               340100    \n",
      "_________________________________________________________________\n",
      "dense_181 (Dense)            (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 1,010,404\n",
      "Trainable params: 511,404\n",
      "Non-trainable params: 499,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "model = Sequential()\n",
    "e =  Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "model.add(e)\n",
    "model.add( Bidirectional( LSTM(100,dropout=0.20),merge_mode='concat'))\n",
    "model.add( Dense(4, activation = 'softmax'))\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samarthgoal/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1476 samples, validate on 492 samples\n",
      "Epoch 1/10\n",
      " - 62s - loss: 1.2721 - acc: 0.3936 - val_loss: 1.0072 - val_acc: 0.5732\n",
      "Epoch 2/10\n",
      " - 3s - loss: 0.9522 - acc: 0.6118 - val_loss: 0.8152 - val_acc: 0.6565\n",
      "Epoch 3/10\n",
      " - 3s - loss: 0.7529 - acc: 0.6999 - val_loss: 0.8005 - val_acc: 0.6992\n",
      "Epoch 4/10\n",
      " - 3s - loss: 0.6760 - acc: 0.7337 - val_loss: 0.7393 - val_acc: 0.7175\n",
      "Epoch 5/10\n",
      " - 3s - loss: 0.6048 - acc: 0.7615 - val_loss: 0.7382 - val_acc: 0.7317\n",
      "Epoch 6/10\n",
      " - 3s - loss: 0.5542 - acc: 0.7873 - val_loss: 0.7525 - val_acc: 0.7378\n",
      "Epoch 7/10\n",
      " - 3s - loss: 0.5025 - acc: 0.8089 - val_loss: 0.7295 - val_acc: 0.7337\n",
      "Epoch 8/10\n",
      " - 3s - loss: 0.4514 - acc: 0.8347 - val_loss: 0.8188 - val_acc: 0.7256\n",
      "Epoch 9/10\n",
      " - 3s - loss: 0.4024 - acc: 0.8516 - val_loss: 0.7636 - val_acc: 0.7500\n",
      "Epoch 10/10\n",
      " - 3s - loss: 0.3880 - acc: 0.8462 - val_loss: 0.7974 - val_acc: 0.7398\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train,validation_split=0.25, nb_epoch = 10, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.593496\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model <100 LSTM two biderctional>\n",
    "loss, accuracy = model.evaluate(X_test, Y_test, verbose=2)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.72      0.73       129\n",
      "           1       0.75      0.64      0.69       106\n",
      "           2       0.76      0.89      0.82       127\n",
      "           3       0.73      0.72      0.72       130\n",
      "\n",
      "   micro avg       0.75      0.75      0.75       492\n",
      "   macro avg       0.75      0.74      0.74       492\n",
      "weighted avg       0.75      0.75      0.74       492\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "Y_pred = model.predict(X_test)\n",
    "y_pred = np.array([np.argmax(pred) for pred in Y_pred])\n",
    "print('  Classification Report:\\n',classification_report(Y_test,y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.341463\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model <simple LSTMlayer>\n",
    "loss, accuracy = model.evaluate(X_test, Y_test, verbose=2)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.68      0.71       129\n",
      "           1       0.61      0.73      0.66       106\n",
      "           2       0.80      0.88      0.84       127\n",
      "           3       0.76      0.62      0.68       130\n",
      "\n",
      "   micro avg       0.73      0.73      0.73       492\n",
      "   macro avg       0.73      0.73      0.72       492\n",
      "weighted avg       0.73      0.73      0.72       492\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "Y_pred = model.predict(X_test)\n",
    "y_pred = np.array([np.argmax(pred) for pred in Y_pred])\n",
    "print('  Classification Report:\\n',classification_report(Y_test,y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### fuse the language and image vectors...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
